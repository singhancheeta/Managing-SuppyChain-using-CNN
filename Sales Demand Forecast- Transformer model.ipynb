{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626a87ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Transformer Model</b> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf9b189",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Dependencies</b> </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac4b4a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.16.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.16.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "#from tensorflow.python.keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "#from tensorflow.python.keras.layers import Dense, LSTM, RepeatVector, TimeDistributed, Flatten\n",
    "from keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import Dense, Conv1D , MaxPooling1D , LSTM, TimeDistributed, Flatten, GRU, RepeatVector\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "import plotly.offline as py #visualization\n",
    "py.init_notebook_mode(connected=True) #visualization\n",
    "import plotly.graph_objs as go #visualization\n",
    "import plotly.tools as tls #visualization\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# Set seeds to make the experiment more reproducible.\n",
    "from tensorflow.compat.v1 import set_random_seed\n",
    "from numpy.random import seed\n",
    "set_random_seed(1)\n",
    "seed(1)\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from RNN import RNN\n",
    "#from CNN import CNN\n",
    "from Transformer import Transformer\n",
    "\n",
    "from utils import series_to_supervised\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47f63526",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68a43d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/Users/moumi/Documents/SCU/Python/jupyter/DL Project/demand-forecasting-kernels-only/train.csv', parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00b6e8f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(913000, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e8e6617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min date from train set: 2013-01-01\n",
      "Max date from train set: 2017-12-31\n"
     ]
    }
   ],
   "source": [
    "#Time period of the train dataset\n",
    "print('Min date from train set: %s' % train['date'].min().date())\n",
    "print('Max date from train set: %s' % train['date'].max().date())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726df1ba",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b> Data Cleaning and Pre-processing</b> </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba38e289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                     datetime64[ns]\n",
       "store                             int64\n",
       "item                              int64\n",
       "sales                             int64\n",
       "year                              int64\n",
       "month                             int64\n",
       "day                               int64\n",
       "week                              int64\n",
       "dayofweek                         int64\n",
       "weekday                           int64\n",
       "dayofyear                         int64\n",
       "quarter                           int64\n",
       "daily_avg                       float64\n",
       "monthly_avg                     float64\n",
       "mean_store_item_month           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding features to the training dataset to granular level\n",
    "train['date']=pd.to_datetime(train['date'], infer_datetime_format=True)\n",
    "train['year'] = train['date'].dt.year\n",
    "train['month'] = train['date'].dt.month\n",
    "train['day'] = train['date'].dt.day\n",
    "train['week'] = train['date'].dt.week\n",
    "train['dayofweek'] = train['date'].dt.dayofweek\n",
    "train['weekday'] = train['date'].dt.weekday\n",
    "train['dayofyear'] = train['date'].dt.dayofyear\n",
    "train['quarter'] = train['date'].dt.quarter\n",
    "train['daily_avg']=train.groupby(['item','store','dayofweek'])['sales'].transform('mean')\n",
    "train['monthly_avg']=train.groupby(['item','store','month'])['sales'].transform('mean')\n",
    "train[\"mean_store_item_month\"] = train.groupby(['month',\"item\",\"store\"])[\"sales\"].transform(\"mean\")\n",
    "daily_avg=train.groupby(['item','store','dayofweek'])['sales'].mean().reset_index()\n",
    "monthly_avg=train.groupby(['item','store','month'])['sales'].mean().reset_index()\n",
    "mean_store_item_month = train.groupby(['month','item','store'])['sales'].mean().reset_index()\n",
    "train.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dedced29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store</th>\n",
       "      <th>item</th>\n",
       "      <th>sales</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>week</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>weekday</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>quarter</th>\n",
       "      <th>daily_avg</th>\n",
       "      <th>monthly_avg</th>\n",
       "      <th>mean_store_item_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18.168582</td>\n",
       "      <td>13.709677</td>\n",
       "      <td>13.709677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>18.793103</td>\n",
       "      <td>13.709677</td>\n",
       "      <td>13.709677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>19.452107</td>\n",
       "      <td>13.709677</td>\n",
       "      <td>13.709677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>21.015326</td>\n",
       "      <td>13.709677</td>\n",
       "      <td>13.709677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>22.973180</td>\n",
       "      <td>13.709677</td>\n",
       "      <td>13.709677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  store  item  sales  year  month  day  week  dayofweek  weekday  \\\n",
       "0 2013-01-01      1     1     13  2013      1    1     1          1        1   \n",
       "1 2013-01-02      1     1     11  2013      1    2     1          2        2   \n",
       "2 2013-01-03      1     1     14  2013      1    3     1          3        3   \n",
       "3 2013-01-04      1     1     13  2013      1    4     1          4        4   \n",
       "4 2013-01-05      1     1     10  2013      1    5     1          5        5   \n",
       "\n",
       "   dayofyear  quarter  daily_avg  monthly_avg  mean_store_item_month  \n",
       "0          1        1  18.168582    13.709677              13.709677  \n",
       "1          2        1  18.793103    13.709677              13.709677  \n",
       "2          3        1  19.452107    13.709677              13.709677  \n",
       "3          4        1  21.015326    13.709677              13.709677  \n",
       "4          5        1  22.973180    13.709677              13.709677  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making a copy of the training data to use to for further processing\n",
    "train_transformer = train.copy()\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a194f85",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Train, Validation and Test Data Preparation</b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3f45839",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating test dataset from training dataset for deep learning models\n",
    "test_transformer = train_transformer[(train_transformer['date'] > '2017-08-01')]\n",
    "train_transformer = train_transformer[(train_transformer['date'] <= '2017-07-31')]\n",
    "#Dropping target column - date column as other custom created date features will be used instead to train the model\n",
    "train_transformer= train_transformer.drop(['date'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "705700cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(836500, 14)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transformer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3df3f648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: 836500\n",
      "Training Observations: 669200\n",
      "Validation Observations: 167300\n",
      "Testing Observations: 76000\n"
     ]
    }
   ],
   "source": [
    "#making a copy of train and test dataset for machine learning models\n",
    "#test_copy = test.copy()\n",
    "#train_copy = train.copy()\n",
    "#test configurations for machine learning models\n",
    "x_test_transformer = test_transformer.drop(['sales', 'date'], axis=1)\n",
    "y_test_transformer = test_transformer.pop('sales')\n",
    "y_date = test_transformer.pop('date')\n",
    "#training configurations for machine learning models and splitting validation data set by 20%\n",
    "x_train,x_val,y_train,y_val = train_test_split(train_copy.drop('sales',axis=1),train_copy.pop('sales'),random_state=42,test_size=0.2)\n",
    "print('Observations: %d' % (len(train_copy)))\n",
    "print('Training Observations: %d' % (len(x_train)))\n",
    "print('Validation Observations: %d' % (len(x_val)))\n",
    "print('Testing Observations: %d' % (len(x_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bf105522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: 836500\n",
      "Training Observations: 669200\n",
      "Validation Observations: 167300\n",
      "Testing Observations: 76000\n"
     ]
    }
   ],
   "source": [
    "# #making a copy of train and test dataset for deep learning models\n",
    "# test_v2_copy = test_v2.copy()\n",
    "# train_v2_copy = train_v2.copy()\n",
    "# #test configurations for deep learning models\n",
    "# x_test_v2 = test_v2_copy.drop(['sales', 'date'], axis=1)\n",
    "# y_test_v2 = test_v2_copy.pop('sales')\n",
    "# y_date_v2 = test_v2_copy.pop('date')\n",
    "# #training configurations for deep learning models and splitting validation data set by 20%\n",
    "# x_train_v2,x_val_v2,y_train_v2,y_val_v2 = train_test_split(train_v2_copy.drop('sales',axis=1),train_v2_copy.pop('sales'),random_state=42,test_size=0.2)\n",
    "# print('Observations: %d' % (len(train_v2_copy)))\n",
    "# print('Training Observations: %d' % (len(x_train_v2)))\n",
    "# print('Validation Observations: %d' % (len(x_val_v2)))\n",
    "# print('Testing Observations: %d' % (len(x_test_v2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b0f28d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#introducing look-back period of 14 days\n",
    "look_back = 14\n",
    "n_features = train_transformer.shape[1]\n",
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc42f64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk-forward data split to avoid data leakage\n",
    "X_train, y_train, X_test, y_test, scale_X = series_to_supervised(train_transformer, train_size=0.8, n_in=look_back, n_out=14, target_column='sales', dropnan=True, scale_X=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dfd4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 14, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 14, 14)      28          ['input_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_4 (MultiH  (None, 14, 14)      60430       ['layer_normalization_8[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 14, 14)       0           ['multi_head_attention_4[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_8 (TFOpLa  (None, 14, 14)      0           ['dropout_9[0][0]',              \n",
      " mbda)                                                            'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 14, 14)      28          ['tf.__operators__.add_8[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)              (None, 14, 4)        60          ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 14, 4)        0           ['conv1d_8[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)              (None, 14, 14)       70          ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_9 (TFOpLa  (None, 14, 14)      0           ['conv1d_9[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add_8[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 14, 14)      28          ['tf.__operators__.add_9[0][0]'] \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_5 (MultiH  (None, 14, 14)      60430       ['layer_normalization_10[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 14, 14)       0           ['multi_head_attention_5[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_10 (TFOpL  (None, 14, 14)      0           ['dropout_11[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_9[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 14, 14)      28          ['tf.__operators__.add_10[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)             (None, 14, 4)        60          ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 14, 4)        0           ['conv1d_10[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)             (None, 14, 14)       70          ['dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_11 (TFOpL  (None, 14, 14)      0           ['conv1d_11[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_10[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 14, 14)      28          ['tf.__operators__.add_11[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_6 (MultiH  (None, 14, 14)      60430       ['layer_normalization_12[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 14, 14)       0           ['multi_head_attention_6[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_12 (TFOpL  (None, 14, 14)      0           ['dropout_13[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_11[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 14, 14)      28          ['tf.__operators__.add_12[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)             (None, 14, 4)        60          ['layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 14, 4)        0           ['conv1d_12[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_13 (Conv1D)             (None, 14, 14)       70          ['dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_13 (TFOpL  (None, 14, 14)      0           ['conv1d_13[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_12[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 14, 14)      28          ['tf.__operators__.add_13[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_7 (MultiH  (None, 14, 14)      60430       ['layer_normalization_14[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 14, 14)       0           ['multi_head_attention_7[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_14 (TFOpL  (None, 14, 14)      0           ['dropout_15[0][0]',             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ambda)                                                           'tf.__operators__.add_13[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 14, 14)      28          ['tf.__operators__.add_14[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_14 (Conv1D)             (None, 14, 4)        60          ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 14, 4)        0           ['conv1d_14[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_15 (Conv1D)             (None, 14, 14)       70          ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_15 (TFOpL  (None, 14, 14)      0           ['conv1d_15[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_14[0][0]']\n",
      "                                                                                                  \n",
      " global_average_pooling1d_1 (Gl  (None, 14)          0           ['tf.__operators__.add_15[0][0]']\n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 128)          1920        ['global_average_pooling1d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 128)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 14)           1806        ['dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 246,190\n",
      "Trainable params: 246,190\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "10456/10456 [==============================] - ETA: 0s - loss: 264.9738 - rmse: 15.2855 - mae: 11.3978 - smape: 12.1543 - coeff_determination: 0.6796WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,rmse,mae,smape,coeff_determination\n",
      "\n",
      "Epoch 1: loss improved from inf to 264.97382, saving model to checkpoint/Transformer.best08122022_19:22:28.hdf5\n",
      "10456/10456 [==============================] - 2760s 263ms/step - loss: 264.9738 - rmse: 15.2855 - mae: 11.3978 - smape: 12.1543 - coeff_determination: 0.6796\n",
      "Epoch 2/20\n",
      "10456/10456 [==============================] - ETA: 0s - loss: 165.4118 - rmse: 12.8215 - mae: 9.5354 - smape: 9.7481 - coeff_determination: 0.7989WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,rmse,mae,smape,coeff_determination\n",
      "\n",
      "Epoch 2: loss improved from 264.97382 to 165.41185, saving model to checkpoint/Transformer.best08122022_19:22:28.hdf5\n",
      "10456/10456 [==============================] - 3920s 375ms/step - loss: 165.4118 - rmse: 12.8215 - mae: 9.5354 - smape: 9.7481 - coeff_determination: 0.7989\n",
      "Epoch 3/20\n",
      "10456/10456 [==============================] - ETA: 0s - loss: 156.7324 - rmse: 12.4846 - mae: 9.3008 - smape: 9.5111 - coeff_determination: 0.8095WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,rmse,mae,smape,coeff_determination\n",
      "\n",
      "Epoch 3: loss improved from 165.41185 to 156.73241, saving model to checkpoint/Transformer.best08122022_19:22:28.hdf5\n",
      "10456/10456 [==============================] - 2952s 282ms/step - loss: 156.7324 - rmse: 12.4846 - mae: 9.3008 - smape: 9.5111 - coeff_determination: 0.8095\n",
      "Epoch 4/20\n",
      "10456/10456 [==============================] - ETA: 0s - loss: 133.6691 - rmse: 11.5135 - mae: 8.5779 - smape: 8.9202 - coeff_determination: 0.8375WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,rmse,mae,smape,coeff_determination\n",
      "\n",
      "Epoch 4: loss improved from 156.73241 to 133.66905, saving model to checkpoint/Transformer.best08122022_19:22:28.hdf5\n",
      "10456/10456 [==============================] - 2942s 281ms/step - loss: 133.6691 - rmse: 11.5135 - mae: 8.5779 - smape: 8.9202 - coeff_determination: 0.8375\n",
      "Epoch 5/20\n",
      "10456/10456 [==============================] - ETA: 0s - loss: 115.7686 - rmse: 10.7232 - mae: 8.0057 - smape: 8.4235 - coeff_determination: 0.8591WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,rmse,mae,smape,coeff_determination\n",
      "\n",
      "Epoch 5: loss improved from 133.66905 to 115.76861, saving model to checkpoint/Transformer.best08122022_19:22:28.hdf5\n",
      "10456/10456 [==============================] - 3035s 290ms/step - loss: 115.7686 - rmse: 10.7232 - mae: 8.0057 - smape: 8.4235 - coeff_determination: 0.8591\n",
      "Epoch 6/20\n",
      "10456/10456 [==============================] - ETA: 0s - loss: 111.4133 - rmse: 10.5220 - mae: 7.8620 - smape: 8.2806 - coeff_determination: 0.8644WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,rmse,mae,smape,coeff_determination\n",
      "\n",
      "Epoch 6: loss improved from 115.76861 to 111.41325, saving model to checkpoint/Transformer.best08122022_19:22:28.hdf5\n",
      "10456/10456 [==============================] - 2742s 262ms/step - loss: 111.4133 - rmse: 10.5220 - mae: 7.8620 - smape: 8.2806 - coeff_determination: 0.8644\n",
      "Epoch 7/20\n",
      " 4757/10456 [============>.................] - ETA: 28:54 - loss: 109.4292 - rmse: 10.4282 - mae: 7.7945 - smape: 8.2113 - coeff_determination: 0.8669"
     ]
    }
   ],
   "source": [
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "X_train_reshaped = X_train.values.reshape((-1,look_back,n_features))\n",
    "X_test_reshaped = X_test.values.reshape((-1,look_back,n_features))\n",
    "\n",
    "y_train_reshaped = y_train.values\n",
    "y_test_reshaped = y_test.values\n",
    "\n",
    "tr = Transformer()\n",
    "tr.train(X_train_reshaped,y_train_reshaped)\n",
    "_, rmse_result, mae_result, smape_result, r2_result = tr.evaluate(X_test_reshaped,y_test_reshaped)\n",
    "\n",
    "\n",
    "print('Result \\n RMSE = %.2f [kWh] \\n MAE = %.2f [kWh]\\n R2 = %.1f [%%]' % (rmse_result,\n",
    "                                                                            mae_result,\n",
    "                                                                            r2_result*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b98f6af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c9186b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bccaa60e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b><h3>Note:</h3></b> </div>\n",
    "\n",
    "We ran the transformer model on google colab with GPU allocation , and were able to successfully train the model on the dataset for 12-14 hours. However, before we could save the model the colab kernel terminated and all the runtime data  got deleted. We tried running the model several time after, after it kept crashing after 7-8 epochs.\n",
    "The model ran had the following test metrics score:\n",
    "\n",
    "Rmse : 8.72\n",
    "Mse: 76.352\n",
    "Mae: 6.625\n",
    "R2_score : 0.907\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d527a6c",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b><h3>Conclusion:</h3></b> </div>\n",
    "    \n",
    "Transformer has a potential to perform better when trained for larger epochs with GPU allocation so that it takes less time to train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef744607",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b><h3>References</h3></b> </div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47880b2b",
   "metadata": {},
   "source": [
    "https://srdas.github.io/DLBook2/\n",
    "https://github.com/mounalab/Multivariate-time-series-forecasting-keras\n",
    "https://towardsdatascience.com/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e\n",
    "https://arxiv.org/abs/2205.13504\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39939e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3252555",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
